{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "u-eSs-jTH39G"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "4UrVcSRNJ8TN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "MpuN6tBeH4j1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Dataset class\n",
        "class PretrainingDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        tokenized = self.tokenizer(\n",
        "            text=text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": tokenized[\"input_ids\"].squeeze(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PLX36EjXIErX"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "DATA_PATH = \"train.csv\"\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "#texts = data[\"full_text\"].values\n",
        "texts = data[\"full_text\"].values[:500]  # Use first 500 samples\n"
      ],
      "metadata": {
        "id": "SHCfZ9fcIHLu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"microsoft/deberta-v3-base\"\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 512\n"
      ],
      "metadata": {
        "id": "LKR06atOINcX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)"
      ],
      "metadata": {
        "id": "i8PBVCowIRGi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "dataset = PretrainingDataset(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cwtWe5IjIW_l"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15,\n",
        ")"
      ],
      "metadata": {
        "id": "Ou2xD_LqIbzT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"masked_lm_experiment\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",  # Disable W&B\n",
        ")\n"
      ],
      "metadata": {
        "id": "PqSXnzgeJgPV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset\n",
        "train_texts, eval_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create evaluation dataset\n",
        "eval_dataset = PretrainingDataset(\n",
        "    texts=eval_texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# Initialize Trainer with eval_dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,  # Add evaluation dataset\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TO4YtGFvIxq5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-953Q5UIdyH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "tnYhyKGZIf6I"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "G-UFIXh3Ih2o",
        "outputId": "ba568811-f487-48ca-eb5c-0f2d191aaa73"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 00:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.323203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.270113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.159868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.155928134918213,\n",
              " 'eval_runtime': 0.8257,\n",
              " 'eval_samples_per_second': 121.111,\n",
              " 'eval_steps_per_second': 15.744,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKk6K9DvJApa"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwPe_ZSjMdsM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txHNnMNPNHRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ab7jt06YNrlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VehStyBLNrnu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_encodings = tokenizer(custom_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "rG_Hgys2Nrp1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "custom_dataset = CustomTextDataset(custom_encodings)\n"
      ],
      "metadata": {
        "id": "dQlNqyKcNrsM"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "custom_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=1)\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculations for inference\n",
        "    for batch in custom_loader:\n",
        "        outputs = model(**batch)\n",
        "        predictions = outputs.logits  # For classification, this will be logits\n",
        "        predicted_class = torch.argmax(predictions, dim=-1)  # Get the predicted class index\n",
        "        print(f\"Prediction: {predicted_class.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyq6TgCINruT",
        "outputId": "38c51d4f-d090-440f-ad65-365f6c6247d5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 1\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neuwjyujNrxp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1dIL8AjNzrB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define custom text inputs\n",
        "custom_texts = [ \"NLP is based on transformers.\"]\n",
        "\n",
        "# Tokenize the custom texts and return attention mask\n",
        "custom_encodings = tokenizer(custom_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "# Explicitly set pad_token_id\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "generated_text = model.generate(\n",
        "    input_ids=custom_encodings['input_ids'],\n",
        "    attention_mask=custom_encodings['attention_mask'],\n",
        "    max_length=50,\n",
        "    pad_token_id=pad_token_id,\n",
        "    top_p=0.9,  # Nucleus sampling (probability threshold)\n",
        "    temperature=0.7,  # Control randomness\n",
        "    no_repeat_ngram_size=2  # Avoid repeating n-grams\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "YaVT9aYcOVlN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfpSp5UFRAtr"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(f\"Generated Text: {decoded_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07f3T_xRJ-C",
        "outputId": "f3d31ca0-b946-4637-c8ab-fb673b360ffc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: NLP is based on transformers.\n",
            "\n",
            "The first step is to create a transformer. The first transformer is a simple one. It is used to transform a single value into a number. For example, if we have a value of 1,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9qz2yoiSdA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}