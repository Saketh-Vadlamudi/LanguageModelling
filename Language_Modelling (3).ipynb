{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "u-eSs-jTH39G"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "4UrVcSRNJ8TN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "MpuN6tBeH4j1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Dataset class\n",
        "class PretrainingDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        tokenized = self.tokenizer(\n",
        "            text=text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": tokenized[\"input_ids\"].squeeze(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PLX36EjXIErX"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "DATA_PATH = \"train.csv\"\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "#texts = data[\"full_text\"].values\n",
        "texts = data[\"full_text\"].values[:8500]  # Use first 500 samples\n"
      ],
      "metadata": {
        "id": "SHCfZ9fcIHLu"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"microsoft/deberta-v3-base\"\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 512\n"
      ],
      "metadata": {
        "id": "LKR06atOINcX"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)"
      ],
      "metadata": {
        "id": "i8PBVCowIRGi"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "dataset = PretrainingDataset(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cwtWe5IjIW_l"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15,\n",
        ")"
      ],
      "metadata": {
        "id": "Ou2xD_LqIbzT"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"masked_lm_experiment\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",  # Disable W&B\n",
        ")\n"
      ],
      "metadata": {
        "id": "PqSXnzgeJgPV"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Split the dataset\n",
        "train_texts, eval_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create evaluation dataset\n",
        "eval_dataset = PretrainingDataset(\n",
        "    texts=eval_texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# Initialize Trainer with eval_dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,  # Add evaluation dataset\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TO4YtGFvIxq5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-953Q5UIdyH"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "tnYhyKGZIf6I"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "G-UFIXh3Ih2o",
        "outputId": "0b6266d1-3997-4207-a60d-9a5ac7284e64"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1467' max='1467' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1467/1467 05:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.055767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.314400</td>\n",
              "      <td>1.994650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.130800</td>\n",
              "      <td>1.936366</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [98/98 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.924033284187317,\n",
              " 'eval_runtime': 6.4069,\n",
              " 'eval_samples_per_second': 122.212,\n",
              " 'eval_steps_per_second': 15.296,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Calculate perplexity from the loss\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "# Example evaluation results\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Get the evaluation loss (log-likelihood)\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = calculate_perplexity(eval_loss)\n",
        "print(\"Perplexity:\", perplexity.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "GvuJ-kWfL_bn",
        "outputId": "4e1bc057-78ee-4372-f2f7-0180d3a04f27"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='196' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [98/98 00:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 6.832748889923096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiQrjxiYJUrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input text with a masked token\n",
        "input_text = \"I am a South [MASK]\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Move the inputs to the same device as the model (GPU or CPU)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Get the model's prediction\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Find the index of the [MASK] token\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Get the top 5 predictions for the [MASK] token\n",
        "predicted_ids = torch.topk(logits[0, mask_token_index], k=5, dim=-1).indices\n",
        "\n",
        "# Decode and print the top predictions\n",
        "for idx, token_id in enumerate(predicted_ids[0]):\n",
        "    predicted_word = tokenizer.decode(token_id)\n",
        "    print(f\"Prediction {idx + 1}: {predicted_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9t0_fwFGyjd",
        "outputId": "e4c59765-6a2e-4340-94c6-c3d20039bd1d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 1: .\n",
            "Prediction 2: american\n",
            "Prediction 3: african\n",
            "Prediction 4: ;\n",
            "Prediction 5: !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define custom text inputs\n",
        "custom_texts = [ \"NLP is based on transformers.\"]\n",
        "\n",
        "# Tokenize the custom texts and return attention mask\n",
        "custom_encodings = tokenizer(custom_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "# Explicitly set pad_token_id\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "generated_text = model.generate(\n",
        "    input_ids=custom_encodings['input_ids'],\n",
        "    attention_mask=custom_encodings['attention_mask'],\n",
        "    max_length=50,\n",
        "    pad_token_id=pad_token_id,\n",
        "    top_p=0.9,  # Nucleus sampling (probability threshold)\n",
        "    temperature=0.7,  # Control randomness\n",
        "    no_repeat_ngram_size=2  # Avoid repeating n-grams\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "YaVT9aYcOVlN"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(f\"Generated Text: {decoded_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07f3T_xRJ-C",
        "outputId": "58f067fb-5ef2-4f3a-d228-d723e5ff7750"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: NLP is based on transformers.\n",
            "\n",
            "The first step is to create a transformer. The first transformer is a simple one. It is used to transform a single value into a number. For example, if we have a value of 1,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Define a custom text with a partial sentence\n",
        "partial_sentence = \"I had breakfast this morning and i loved the way it\"\n",
        "\n",
        "# Tokenize the partial sentence\n",
        "input_ids = tokenizer(partial_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Get the model output logits\n",
        "outputs = model(input_ids=input_ids)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Extract the logits for the last token\n",
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probs = softmax(last_token_logits, dim=-1)\n",
        "\n",
        "# Get the top 10 possible next words\n",
        "top_k = 10\n",
        "top_k_indices = torch.topk(probs, top_k).indices\n",
        "top_k_words = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top possible next words:\")\n",
        "for word, prob in zip(top_k_words, probs[top_k_indices]):\n",
        "    print(f\"{word}: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Y9qz2yoiSdA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7885c689-e203-49d2-e6d2-acbae4c0f61c"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top possible next words:\n",
            " looked: 0.1540\n",
            " was: 0.1375\n",
            " turned: 0.1069\n",
            " tasted: 0.0813\n",
            " smelled: 0.0789\n",
            " cooked: 0.0390\n",
            " came: 0.0249\n",
            " made: 0.0224\n",
            " reminded: 0.0174\n",
            " smells: 0.0108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K8xFftO-8koe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "xprjc5v_9fuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01rGX1li9XZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUxN9AM9DEc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl2sU5m4DE_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}