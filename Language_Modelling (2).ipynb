{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "u-eSs-jTH39G"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "4UrVcSRNJ8TN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "MpuN6tBeH4j1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Dataset class\n",
        "class PretrainingDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        tokenized = self.tokenizer(\n",
        "            text=text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": tokenized[\"input_ids\"].squeeze(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PLX36EjXIErX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "DATA_PATH = \"train.csv\"\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "#texts = data[\"full_text\"].values\n",
        "texts = data[\"full_text\"].values[:500]  # Use first 500 samples\n"
      ],
      "metadata": {
        "id": "SHCfZ9fcIHLu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"microsoft/deberta-v3-base\"\n",
        "'''\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 512\n"
      ],
      "metadata": {
        "id": "LKR06atOINcX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME_OR_PATH)"
      ],
      "metadata": {
        "id": "i8PBVCowIRGi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "dataset = PretrainingDataset(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cwtWe5IjIW_l"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15,\n",
        ")"
      ],
      "metadata": {
        "id": "Ou2xD_LqIbzT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"masked_lm_experiment\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",  # Disable W&B\n",
        ")\n"
      ],
      "metadata": {
        "id": "PqSXnzgeJgPV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset\n",
        "train_texts, eval_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create evaluation dataset\n",
        "eval_dataset = PretrainingDataset(\n",
        "    texts=eval_texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# Initialize Trainer with eval_dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,  # Add evaluation dataset\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TO4YtGFvIxq5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-953Q5UIdyH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "tnYhyKGZIf6I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "G-UFIXh3Ih2o",
        "outputId": "4805c9aa-b5ed-42fc-a6f1-2fcd84fb68e8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 01:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.324391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.266984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.161151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.156226396560669,\n",
              " 'eval_runtime': 2.2277,\n",
              " 'eval_samples_per_second': 44.89,\n",
              " 'eval_steps_per_second': 5.836,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and move model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiQrjxiYJUrQ",
        "outputId": "52b88a78-e26f-4a2c-8411-a429679727f1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForMaskedLM(\n",
              "  (activation): GELUActivation()\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
              "  (mlm_loss_fct): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input text with a masked token\n",
        "input_text = \"I am a south [MASK]\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Move the inputs to the same device as the model (GPU or CPU)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Get the model's prediction\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Find the index of the [MASK] token\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Get the top 5 predictions for the [MASK] token\n",
        "predicted_ids = torch.topk(logits[0, mask_token_index], k=5, dim=-1).indices\n",
        "\n",
        "# Decode and print the top predictions\n",
        "for idx, token_id in enumerate(predicted_ids[0]):\n",
        "    predicted_word = tokenizer.decode(token_id)\n",
        "    print(f\"Prediction {idx + 1}: {predicted_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9t0_fwFGyjd",
        "outputId": "8da3439c-a663-443f-e444-472872abc274"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 1: .\n",
            "Prediction 2: ;\n",
            "Prediction 3: !\n",
            "Prediction 4: african\n",
            "Prediction 5: american\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define custom text inputs\n",
        "custom_texts = [ \"NLP is based on transformers.\"]\n",
        "\n",
        "# Tokenize the custom texts and return attention mask\n",
        "custom_encodings = tokenizer(custom_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "# Explicitly set pad_token_id\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "generated_text = model.generate(\n",
        "    input_ids=custom_encodings['input_ids'],\n",
        "    attention_mask=custom_encodings['attention_mask'],\n",
        "    max_length=50,\n",
        "    pad_token_id=pad_token_id,\n",
        "    top_p=0.9,  # Nucleus sampling (probability threshold)\n",
        "    temperature=0.7,  # Control randomness\n",
        "    no_repeat_ngram_size=2  # Avoid repeating n-grams\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "YaVT9aYcOVlN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(f\"Generated Text: {decoded_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07f3T_xRJ-C",
        "outputId": "ec3ca966-b873-4fda-8b05-8e9a93923318"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: NLP is based on transformers.\n",
            "\n",
            "The first step is to create a transformer. The first transformer is a simple one. It is used to transform a single value into a number. For example, if we have a value of 1,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Define a custom text with a partial sentence\n",
        "partial_sentence = \"I had breakfast this morning and i loved the way it\"\n",
        "\n",
        "# Tokenize the partial sentence\n",
        "input_ids = tokenizer(partial_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Get the model output logits\n",
        "outputs = model(input_ids=input_ids)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Extract the logits for the last token\n",
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probs = softmax(last_token_logits, dim=-1)\n",
        "\n",
        "# Get the top 10 possible next words\n",
        "top_k = 10\n",
        "top_k_indices = torch.topk(probs, top_k).indices\n",
        "top_k_words = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top possible next words:\")\n",
        "for word, prob in zip(top_k_words, probs[top_k_indices]):\n",
        "    print(f\"{word}: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Y9qz2yoiSdA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c77f7d-0c6c-45e1-c5cd-632a00c96b5e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top possible next words:\n",
            " looked: 0.1540\n",
            " was: 0.1375\n",
            " turned: 0.1069\n",
            " tasted: 0.0813\n",
            " smelled: 0.0789\n",
            " cooked: 0.0390\n",
            " came: 0.0249\n",
            " made: 0.0224\n",
            " reminded: 0.0174\n",
            " smells: 0.0108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K8xFftO-8koe"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "xprjc5v_9fuu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "01rGX1li9XZK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUxN9AM9DEc1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl2sU5m4DE_a"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}